{"cells":[{"cell_type":"markdown","source":["# Extracting duration in seconds from `contentDetails_duration`\n\nIn this notebook we will be working with the `songs` table.  \nIn this table, there is a `contentDetails_duration` that states the duration of the song. Our issue is that the format is not readable for analysis or modelisation.\nThe goal of this notebook is to convert it into seconds."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ef0abdc9-e2be-4a37-ae94-97a9d6555f05"}}},{"cell_type":"markdown","source":["## Loading data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12f17da8-1c0b-4f76-a63b-7f29bd3e9b38"}}},{"cell_type":"code","source":["filepath = \"s3://jedha-cloud-storage-lucy/youtube-/interim/\"\naws_access_key_id=\"-\"\naws_secret_access_key=\"-\"\nhadoop_conf = spark._jsc.hadoopConfiguration()\nhadoop_conf.set(\"fs.s3n.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\nhadoop_conf.set(\"fs.s3n.awsAccessKeyId\", aws_access_key_id)\nhadoop_conf.set(\"fs.s3n.awsSecretAccessKey\", aws_secret_access_key)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c1f0b0a4-9242-4896-aa8d-51e20a040c2f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# TODO: Load the file into a PySpark DataFrame\n#       Perform the usual checks\n### BEGIN STRIP ###\nsongs = spark.read.load (filepath + 'items_selected.parquet')\n#playlog.printSchema()\nsongs.limit(10).toPandas()\n### END STRIP ###"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf960588-0bd9-42c0-9d5f-9422b5323cf6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"songs","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"contentDetails_duration","nullable":true,"type":"string"},{"metadata":{},"name":"id","nullable":true,"type":"string"},{"metadata":{},"name":"snippet_channelId","nullable":true,"type":"string"},{"metadata":{},"name":"snippet_channelTitle","nullable":true,"type":"string"},{"metadata":{},"name":"snippet_publishedAt","nullable":true,"type":"string"},{"metadata":{"redshift_type":"VARCHAR(512)"},"name":"snippet_title","nullable":true,"type":"string"},{"metadata":{},"name":"statistics_commentCount","nullable":true,"type":"long"},{"metadata":{},"name":"statistics_dislikeCount","nullable":true,"type":"long"},{"metadata":{},"name":"statistics_viewCount","nullable":true,"type":"long"}],"type":"struct"},"tableIdentifier":null}],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>contentDetails_duration</th>\n      <th>id</th>\n      <th>snippet_channelId</th>\n      <th>snippet_channelTitle</th>\n      <th>snippet_publishedAt</th>\n      <th>snippet_title</th>\n      <th>statistics_commentCount</th>\n      <th>statistics_dislikeCount</th>\n      <th>statistics_viewCount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>PT3M33S</td>\n      <td>t1l8Z6gLPzo</td>\n      <td>UCUERSOitwgUq_37kGslN96w</td>\n      <td>VOLO</td>\n      <td>2013-07-22T12:09:11Z</td>\n      <td>VOLO. \"L'air d'un con\"</td>\n      <td>38</td>\n      <td>26</td>\n      <td>223172</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>PT7M46S</td>\n      <td>we5gzZq5Avg</td>\n      <td>UCson549gpvRhPnJ3Whs5onA</td>\n      <td>LongWayToDream</td>\n      <td>2012-03-17T08:34:30Z</td>\n      <td>Julian Jeweil - Air Conditionné</td>\n      <td>2</td>\n      <td>3</td>\n      <td>13409</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>PT3M7S</td>\n      <td>49esza4eiK4</td>\n      <td>UCcHYZ8Ez4gG_2bHEuBL8IfQ</td>\n      <td>Downtown Records</td>\n      <td>2007-09-08T02:02:07Z</td>\n      <td>Justice - D.A.N.C.E</td>\n      <td>3168</td>\n      <td>780</td>\n      <td>10106655</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>PT3M43S</td>\n      <td>BoO6LfR7ca0</td>\n      <td>UCQ0wLCF7u23gZKJkHFs1Tpg</td>\n      <td>Music Is Our Drug</td>\n      <td>2014-01-24T12:52:38Z</td>\n      <td>Gramatik - Torture (feat. Eric Krasno)</td>\n      <td>6</td>\n      <td>0</td>\n      <td>29153</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>PT5M</td>\n      <td>DaH4W1rY9us</td>\n      <td>UCJsTMPZxYD-Q3kEmL4Qijpg</td>\n      <td>Harvey Pearson</td>\n      <td>2012-12-02T12:41:13Z</td>\n      <td>Ben Howard - Oats In The Water</td>\n      <td>5303</td>\n      <td>1784</td>\n      <td>16488714</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>PT10M16S</td>\n      <td>PAAUqBghiVo</td>\n      <td>UCDHvlud7Hf86FxFsogrBcMg</td>\n      <td>Resident Advisor</td>\n      <td>2013-10-15T09:30:25Z</td>\n      <td>RA Sessions: DARKSIDE - Paper Trails</td>\n      <td>883</td>\n      <td>445</td>\n      <td>3381880</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>PT7M59S</td>\n      <td>7CkBU80AsVg</td>\n      <td>UCr7vKEGOLRCf4ivVzUcK6dw</td>\n      <td>Matheus Castilho</td>\n      <td>2010-12-18T19:19:36Z</td>\n      <td>Dusty Kid - America</td>\n      <td>10</td>\n      <td>6</td>\n      <td>29520</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>PT3M34S</td>\n      <td>lddHsCBdQu8</td>\n      <td>UCX_fIhJAag_NvUN7I1dwjBg</td>\n      <td>MOMOMOYOUTH</td>\n      <td>2013-05-15T08:19:20Z</td>\n      <td>MØ - Waste Of Time (Official Audio)</td>\n      <td>260</td>\n      <td>96</td>\n      <td>750787</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>PT3M58S</td>\n      <td>mhsf7K6h7SY</td>\n      <td>UCJ6td3C9QlPO9O_J5dF4ZzA</td>\n      <td>Monstercat: Uncaged</td>\n      <td>2014-01-15T18:33:15Z</td>\n      <td>[Electronic] - Pegboard Nerds - Bassline Kicki...</td>\n      <td>2512</td>\n      <td>780</td>\n      <td>1719694</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>PT1M7S</td>\n      <td>3go6xFb0FrU</td>\n      <td>UCPoG8-NKfLQ23kmWX2XKDwQ</td>\n      <td>WassupTeam</td>\n      <td>2009-10-01T20:53:09Z</td>\n      <td>Always Coca Cola A cappella</td>\n      <td>18</td>\n      <td>11</td>\n      <td>30457</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":"<div class=\"ansiout\">Out[4]: </div>","removedWidgets":[],"addedWidgets":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>contentDetails_duration</th>\n      <th>id</th>\n      <th>snippet_channelId</th>\n      <th>snippet_channelTitle</th>\n      <th>snippet_publishedAt</th>\n      <th>snippet_title</th>\n      <th>statistics_commentCount</th>\n      <th>statistics_dislikeCount</th>\n      <th>statistics_viewCount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>PT3M33S</td>\n      <td>t1l8Z6gLPzo</td>\n      <td>UCUERSOitwgUq_37kGslN96w</td>\n      <td>VOLO</td>\n      <td>2013-07-22T12:09:11Z</td>\n      <td>VOLO. \"L'air d'un con\"</td>\n      <td>38</td>\n      <td>26</td>\n      <td>223172</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>PT7M46S</td>\n      <td>we5gzZq5Avg</td>\n      <td>UCson549gpvRhPnJ3Whs5onA</td>\n      <td>LongWayToDream</td>\n      <td>2012-03-17T08:34:30Z</td>\n      <td>Julian Jeweil - Air Conditionné</td>\n      <td>2</td>\n      <td>3</td>\n      <td>13409</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>PT3M7S</td>\n      <td>49esza4eiK4</td>\n      <td>UCcHYZ8Ez4gG_2bHEuBL8IfQ</td>\n      <td>Downtown Records</td>\n      <td>2007-09-08T02:02:07Z</td>\n      <td>Justice - D.A.N.C.E</td>\n      <td>3168</td>\n      <td>780</td>\n      <td>10106655</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>PT3M43S</td>\n      <td>BoO6LfR7ca0</td>\n      <td>UCQ0wLCF7u23gZKJkHFs1Tpg</td>\n      <td>Music Is Our Drug</td>\n      <td>2014-01-24T12:52:38Z</td>\n      <td>Gramatik - Torture (feat. Eric Krasno)</td>\n      <td>6</td>\n      <td>0</td>\n      <td>29153</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>PT5M</td>\n      <td>DaH4W1rY9us</td>\n      <td>UCJsTMPZxYD-Q3kEmL4Qijpg</td>\n      <td>Harvey Pearson</td>\n      <td>2012-12-02T12:41:13Z</td>\n      <td>Ben Howard - Oats In The Water</td>\n      <td>5303</td>\n      <td>1784</td>\n      <td>16488714</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>PT10M16S</td>\n      <td>PAAUqBghiVo</td>\n      <td>UCDHvlud7Hf86FxFsogrBcMg</td>\n      <td>Resident Advisor</td>\n      <td>2013-10-15T09:30:25Z</td>\n      <td>RA Sessions: DARKSIDE - Paper Trails</td>\n      <td>883</td>\n      <td>445</td>\n      <td>3381880</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>PT7M59S</td>\n      <td>7CkBU80AsVg</td>\n      <td>UCr7vKEGOLRCf4ivVzUcK6dw</td>\n      <td>Matheus Castilho</td>\n      <td>2010-12-18T19:19:36Z</td>\n      <td>Dusty Kid - America</td>\n      <td>10</td>\n      <td>6</td>\n      <td>29520</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>PT3M34S</td>\n      <td>lddHsCBdQu8</td>\n      <td>UCX_fIhJAag_NvUN7I1dwjBg</td>\n      <td>MOMOMOYOUTH</td>\n      <td>2013-05-15T08:19:20Z</td>\n      <td>MØ - Waste Of Time (Official Audio)</td>\n      <td>260</td>\n      <td>96</td>\n      <td>750787</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>PT3M58S</td>\n      <td>mhsf7K6h7SY</td>\n      <td>UCJ6td3C9QlPO9O_J5dF4ZzA</td>\n      <td>Monstercat: Uncaged</td>\n      <td>2014-01-15T18:33:15Z</td>\n      <td>[Electronic] - Pegboard Nerds - Bassline Kicki...</td>\n      <td>2512</td>\n      <td>780</td>\n      <td>1719694</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>PT1M7S</td>\n      <td>3go6xFb0FrU</td>\n      <td>UCPoG8-NKfLQ23kmWX2XKDwQ</td>\n      <td>WassupTeam</td>\n      <td>2009-10-01T20:53:09Z</td>\n      <td>Always Coca Cola A cappella</td>\n      <td>18</td>\n      <td>11</td>\n      <td>30457</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## First analysis"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e03a90b-f793-4e7f-947b-88910271bfb1"}}},{"cell_type":"code","source":["# We will be using this column a lot\nDURATION_COL = 'contentDetails_duration'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"091ee691-e25e-4b1a-83e7-ae10c6eb2cc3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# TODO: show the first 10 values of the `contentDetails_duration` column\n### BEGIN STRIP ###\nsongs.select(DURATION_COL).show(10)\n### END STRIP ###"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0aa18587-e4f0-4de9-87b3-88f8f6c61fd2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----------------------+\n|contentDetails_duration|\n+-----------------------+\n|                PT3M33S|\n|                PT7M46S|\n|                 PT3M7S|\n|                PT3M43S|\n|                   PT5M|\n|               PT10M16S|\n|                PT7M59S|\n|                PT3M34S|\n|                PT3M58S|\n|                 PT1M7S|\n+-----------------------+\nonly showing top 10 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------------+\ncontentDetails_duration|\n+-----------------------+\n                PT3M33S|\n                PT7M46S|\n                 PT3M7S|\n                PT3M43S|\n                   PT5M|\n               PT10M16S|\n                PT7M59S|\n                PT3M34S|\n                PT3M58S|\n                 PT1M7S|\n+-----------------------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# TODO: Convert the duration column to a unix timestamp\n#       then show the first 20 rows (select both original duration and converted duration aliased to `totalSeconds`)\n# NOTE: Be careful, you need to escape some characters\n### BEGIN STRIP ###\nfrom pyspark.sql import functions as F\ntime_format = \"'PT'mm'M'ss'S'\"\nsongs.select(DURATION_COL, F.unix_timestamp(DURATION_COL, time_format).alias('totalSeconds')).show()\n### END STRIP ###"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"840606d0-bb94-45dd-850a-f587997371fb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2518071720507930&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> <span class=\"ansi-green-fg\">from</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql <span class=\"ansi-green-fg\">import</span> functions <span class=\"ansi-green-fg\">as</span> F\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> time_format <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#34;&#39;PT&#39;mm&#39;M&#39;ss&#39;S&#39;&#34;</span>\n<span class=\"ansi-green-fg\">----&gt; 7</span><span class=\"ansi-red-fg\"> </span>songs<span class=\"ansi-blue-fg\">.</span>select<span class=\"ansi-blue-fg\">(</span>DURATION_COL<span class=\"ansi-blue-fg\">,</span> F<span class=\"ansi-blue-fg\">.</span>unix_timestamp<span class=\"ansi-blue-fg\">(</span>DURATION_COL<span class=\"ansi-blue-fg\">,</span> time_format<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>alias<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;totalSeconds&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      8</span> <span class=\"ansi-red-fg\">### END STRIP ###</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">show</span><span class=\"ansi-blue-fg\">(self, n, truncate, vertical)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    439</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    440</span>         <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">,</span> bool<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">and</span> truncate<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 441</span><span class=\"ansi-red-fg\">             </span>print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">20</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    442</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    443</span>             print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> int<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1304</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1305</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1307</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    126</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 127</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    128</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    129</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o320.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 8, ip-10-172-232-85.us-west-2.compute.internal, executor driver): org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to parse &#39;PT3M33S&#39; in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:150)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:141)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.$anonfun$parse$1(TimestampFormatter.scala:87)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:78)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:731)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:187)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:657)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:660)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.time.format.DateTimeParseException: Text &#39;PT3M33S&#39; could not be parsed at index 2\n\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1949)\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.$anonfun$parse$1(TimestampFormatter.scala:79)\n\t... 18 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2478)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2427)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2426)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2426)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1131)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1131)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1131)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2678)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2625)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2613)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:917)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2313)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:298)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:308)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:88)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:508)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollectResult(limit.scala:58)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:2986)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3692)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2710)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3684)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:116)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:248)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:198)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3682)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2710)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2917)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:304)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:341)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to parse &#39;PT3M33S&#39; in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:150)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:141)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.$anonfun$parse$1(TimestampFormatter.scala:87)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:78)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:731)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:187)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:657)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:660)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.time.format.DateTimeParseException: Text &#39;PT3M33S&#39; could not be parsed at index 2\n\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1949)\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.$anonfun$parse$1(TimestampFormatter.scala:79)\n\t... 18 more\n</div>","errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 8, ip-10-172-232-85.us-west-2.compute.internal, executor driver): org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to parse &#39;PT3M33S&#39; in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2518071720507930&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> <span class=\"ansi-green-fg\">from</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql <span class=\"ansi-green-fg\">import</span> functions <span class=\"ansi-green-fg\">as</span> F\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> time_format <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#34;&#39;PT&#39;mm&#39;M&#39;ss&#39;S&#39;&#34;</span>\n<span class=\"ansi-green-fg\">----&gt; 7</span><span class=\"ansi-red-fg\"> </span>songs<span class=\"ansi-blue-fg\">.</span>select<span class=\"ansi-blue-fg\">(</span>DURATION_COL<span class=\"ansi-blue-fg\">,</span> F<span class=\"ansi-blue-fg\">.</span>unix_timestamp<span class=\"ansi-blue-fg\">(</span>DURATION_COL<span class=\"ansi-blue-fg\">,</span> time_format<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>alias<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;totalSeconds&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      8</span> <span class=\"ansi-red-fg\">### END STRIP ###</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">show</span><span class=\"ansi-blue-fg\">(self, n, truncate, vertical)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    439</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    440</span>         <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">,</span> bool<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">and</span> truncate<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 441</span><span class=\"ansi-red-fg\">             </span>print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">20</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    442</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    443</span>             print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> int<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1304</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1305</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1307</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    126</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 127</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    128</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    129</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o320.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 8, ip-10-172-232-85.us-west-2.compute.internal, executor driver): org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to parse &#39;PT3M33S&#39; in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:150)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:141)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.$anonfun$parse$1(TimestampFormatter.scala:87)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:78)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:731)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:187)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:657)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:660)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.time.format.DateTimeParseException: Text &#39;PT3M33S&#39; could not be parsed at index 2\n\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1949)\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.$anonfun$parse$1(TimestampFormatter.scala:79)\n\t... 18 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2478)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2427)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2426)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2426)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1131)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1131)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1131)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2678)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2625)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2613)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:917)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2313)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:298)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:308)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:88)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:508)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollectResult(limit.scala:58)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:2986)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3692)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2710)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3684)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:116)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:248)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:198)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3682)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2710)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2917)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:304)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:341)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to parse &#39;PT3M33S&#39; in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:150)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:141)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.$anonfun$parse$1(TimestampFormatter.scala:87)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:78)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:731)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:187)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:657)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:660)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.time.format.DateTimeParseException: Text &#39;PT3M33S&#39; could not be parsed at index 2\n\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1949)\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.$anonfun$parse$1(TimestampFormatter.scala:79)\n\t... 18 more\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Scroll down and look. Can you see anything weird?  \nWe have null values for `duration_format`, that indicates that our conversion didn't work."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0c2e6acb-70df-47fc-8366-6b153d5ac01d"}}},{"cell_type":"markdown","source":["**[TODO]:** Try to explain why..\n### BEGIN STRIP ###\nBecause the format is different than the one we're using...\n### END STRIP ###"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8f2b0e37-d224-4720-b861-51ff78aa40ad"}}},{"cell_type":"markdown","source":["We'll try to evaluate how many different formats we have. It would be difficult to do this precisely, but a ballmark estimate will do.  \nOur strategy will be to compute the length of each `duration` and count how many of these we got.  \nThen, for each we will select 3 samples to get a better sense of the kind of formats we're dealing with."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e3ef254-c1b9-464a-a556-4989bad07324"}}},{"cell_type":"code","source":["pip install cytoolz"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"78970125-eb29-43ff-939a-2d54c01dac1f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting cytoolz\n  Downloading cytoolz-0.11.0.tar.gz (477 kB)\nCollecting toolz&gt;=0.8.0\n  Downloading toolz-0.11.1-py3-none-any.whl (55 kB)\nBuilding wheels for collected packages: cytoolz\n  Building wheel for cytoolz (setup.py): started\n  Building wheel for cytoolz (setup.py): finished with status &#39;done&#39;\n  Created wheel for cytoolz: filename=cytoolz-0.11.0-cp37-cp37m-linux_x86_64.whl size=1245519 sha256=8ba140e065a58a66f814a72d0aad2a3a575ab2d99b9533e3da3b671de3797f97\n  Stored in directory: /root/.cache/pip/wheels/a1/aa/85/7dc0547cad8c535a1c4e55c638f6e2d3de45401dd634bff0a2\nSuccessfully built cytoolz\nInstalling collected packages: toolz, cytoolz\nSuccessfully installed cytoolz-0.11.0 toolz-0.11.1\nPython interpreter will be restarted.\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting cytoolz\n  Downloading cytoolz-0.11.0.tar.gz (477 kB)\nCollecting toolz&gt;=0.8.0\n  Downloading toolz-0.11.1-py3-none-any.whl (55 kB)\nBuilding wheels for collected packages: cytoolz\n  Building wheel for cytoolz (setup.py): started\n  Building wheel for cytoolz (setup.py): finished with status &#39;done&#39;\n  Created wheel for cytoolz: filename=cytoolz-0.11.0-cp37-cp37m-linux_x86_64.whl size=1245519 sha256=8ba140e065a58a66f814a72d0aad2a3a575ab2d99b9533e3da3b671de3797f97\n  Stored in directory: /root/.cache/pip/wheels/a1/aa/85/7dc0547cad8c535a1c4e55c638f6e2d3de45401dd634bff0a2\nSuccessfully built cytoolz\nInstalling collected packages: toolz, cytoolz\nSuccessfully installed cytoolz-0.11.0 toolz-0.11.1\nPython interpreter will be restarted.\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# TODO: Follow previous instructions\n### BEGIN STRIP ###\nfrom cytoolz import curry\n\n@curry\ndef lengthAnalysis(df, *, length=3):\n  return df.withColumn('duration_length', F.length(DURATION_COL)) \\\n           .groupBy('duration_length') \\\n           .agg(\n             F.count('*'),\n             F.slice(F.collect_list(DURATION_COL), start=1, length=length).alias('values')) \\\n           .orderBy('duration_length')\n\nsongs.transform(lengthAnalysis).rdd.map(lambda r: tuple(r)).collect()\n### END STRIP ###"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9343ee47-c616-40c0-a906-8e7048e31b0e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[7]: [(4, 66, [&#39;PT5M&#39;, &#39;PT3M&#39;, &#39;PT3M&#39;]),\n (5, 6, [&#39;PT58S&#39;, &#39;PT11M&#39;, &#39;PT10M&#39;]),\n (6, 596, [&#39;PT3M7S&#39;, &#39;PT1M7S&#39;, &#39;PT6M2S&#39;]),\n (7, 3165, [&#39;PT3M33S&#39;, &#39;PT7M46S&#39;, &#39;PT3M43S&#39;]),\n (8, 61, [&#39;PT10M16S&#39;, &#39;PT53M59S&#39;, &#39;PT29M49S&#39;]),\n (9, 4, [&#39;PT2H1M59S&#39;, &#39;PT1H9M50S&#39;, &#39;PT1H15M4S&#39;]),\n (10, 9, [&#39;PT1H26M17S&#39;, &#39;PT1H15M11S&#39;, &#39;PT1H19M49S&#39;])]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[7]: [(4, 66, [&#39;PT5M&#39;, &#39;PT3M&#39;, &#39;PT3M&#39;]),\n (5, 6, [&#39;PT58S&#39;, &#39;PT11M&#39;, &#39;PT10M&#39;]),\n (6, 596, [&#39;PT3M7S&#39;, &#39;PT1M7S&#39;, &#39;PT6M2S&#39;]),\n (7, 3165, [&#39;PT3M33S&#39;, &#39;PT7M46S&#39;, &#39;PT3M43S&#39;]),\n (8, 61, [&#39;PT10M16S&#39;, &#39;PT53M59S&#39;, &#39;PT29M49S&#39;]),\n (9, 4, [&#39;PT2H1M59S&#39;, &#39;PT1H9M50S&#39;, &#39;PT1H15M4S&#39;]),\n (10, 9, [&#39;PT1H26M17S&#39;, &#39;PT1H15M11S&#39;, &#39;PT1H19M49S&#39;])]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["That's many! And we're just sampling, there could be more...  \nNow is probably a good time to go and look at some documentation. It appears this time format for duration is following the ISO8601 standard.  \nTake a look at the [wikipedia page](http://en.wikipedia.org/wiki/ISO_8601#Durations).\n\nWe will first try doing this with Python, and will then solve it with PySpark.  \nUsing standard Python's library wouldn't be easy, unless you know about regexes.  \nFor now, we will make it simpler by using an external library: [isodate](https://github.com/gweis/isodate/).\n\nWe will first start by selecting a sample of different formats and make sure our python implementation work on these before shifting to PySpark using UDF.  \nAnd at the end of the notebook, as a bonus, you can try to do it using pure PySpark functions."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a6d66c09-ae89-4313-9b11-8ed73480f1b5"}}},{"cell_type":"markdown","source":["We will first build a sample of the different kind of format we can encouter, for each different length of format, collect 10 (some will have le) different values and store them as a python list called `samples`.  \nIf you made a function for the previous assignment, you can probably reuse it here."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5aea8003-0c50-47fa-8d51-f0b7d72cd5e0"}}},{"cell_type":"code","source":["# TODO: Build a list of sample of each length of format: `samples`\n#       This time, pick 10 of each (when possible)\n#       Then, print out the last 5 values\n### BEGIN STRIP ###\nsamples = songs.transform(lengthAnalysis(length=10)) \\\n               .select(F.explode('values')) \\\n               .distinct() \\\n               .rdd.map(lambda r: r[0]).collect()\nsamples[-5:]\n### END STRIP ###"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ea13ed8-81ed-4b79-bf86-1e4275142a0f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[8]: [&#39;PT1H23M48S&#39;, &#39;PT11M57S&#39;, &#39;PT4M3S&#39;, &#39;PT1H26M17S&#39;, &#39;PT49M39S&#39;]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[8]: [&#39;PT1H23M48S&#39;, &#39;PT11M57S&#39;, &#39;PT4M3S&#39;, &#39;PT1H26M17S&#39;, &#39;PT49M39S&#39;]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Parsing with `isodate`\nWe will use the Python's library [isodate](https://github.com/gweis/isodate/) to help us parse these durations in ISO8601 format.  \nOnce we succeed doing it with regular Python, we can embed this into a PySpark UDF."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2f8db2cb-2216-419b-b88c-10b079b9e51f"}}},{"cell_type":"code","source":["# TODO: install isodate and import it\n### BEGIN STRIP ###\n!pip install isodate\nimport isodate\n### END STRIP ###"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79fc8547-ffd2-454b-a105-cca0a1f0fd79"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Now we will create a function that uses `isodate` to parse an ISO8601 duration and convert it to seconds."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8bdcc61b-84be-455b-b210-4cd341ce20bc"}}},{"cell_type":"code","source":["# TODO: Create a function that parse a duration as a ISO6601 string: `total_seconds_from_ISO8601_duration`\n### BEGIN STRIP ###\ndef total_seconds_from_ISO8601_duration(duration_ISO8601: str) -> float:\n  return isodate.parse_duration(duration_ISO8601).total_seconds()\n### END STRIP ###"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2616d16a-38c8-4754-9c1e-22a7f3e33a47"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# TODO: convert your sample to seconds using your newly created function: `samples_seconds`\n# NOTE: use a list comprehension\n### BEGIN STRIP ###\nsamples_seconds = [total_seconds_from_ISO8601_duration(d) for d in samples]\nsamples_seconds[:5]\n### END STRIP ###"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6d76307e-1e02-486d-a36e-3915d5da8259"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[13]: [3058.0, 900.0, 241.0, 4511.0, 479.0]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[13]: [3058.0, 900.0, 241.0, 4511.0, 479.0]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# TODO: make sure we have no null values (e.g. count the null values and make sure it sums to 0)\n### BEGIN STRIP ###\nsum(e is None for e in samples_seconds)\n### END STRIP ###"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1047829-41ce-48a1-952a-6d4d22017828"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[14]: 0</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[14]: 0</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Using PySpark\nThat seems to be working. We'll try to use this with PySpark now."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f114fe5-6551-451a-9c49-d3a675ccf5ff"}}},{"cell_type":"code","source":["# TODO: Convert your function to an UDF: `total_seconds_from_ISO8601_duration_udf`\n# NOTE: Beware of the return type\n### BEGIN STRIP ###\nfrom pyspark.sql.types import FloatType\n\ntotal_seconds_from_ISO8601_duration_udf = F.udf(total_seconds_from_ISO8601_duration, FloatType())\n### END STRIP ###"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"737d68d0-9ae6-4b89-8d96-e82848c7f215"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# TODO: Using your previously defined UDF, add a new column `totalDurationSeconds`: `songs_output`\n### BEGIN STRIP ###\nsongs_output = songs.withColumn('totalDurationSeconds', total_seconds_from_ISO8601_duration_udf(DURATION_COL))\nsongs_output.limit(5).toPandas()\n### END STRIP ###"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54f6b59a-43c8-4a27-b4c3-d4f044e6527f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"songs_output","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"contentDetails_duration","nullable":true,"type":"string"},{"metadata":{},"name":"id","nullable":true,"type":"string"},{"metadata":{},"name":"snippet_channelId","nullable":true,"type":"string"},{"metadata":{},"name":"snippet_channelTitle","nullable":true,"type":"string"},{"metadata":{},"name":"snippet_publishedAt","nullable":true,"type":"string"},{"metadata":{"redshift_type":"VARCHAR(512)"},"name":"snippet_title","nullable":true,"type":"string"},{"metadata":{},"name":"statistics_commentCount","nullable":true,"type":"long"},{"metadata":{},"name":"statistics_dislikeCount","nullable":true,"type":"long"},{"metadata":{},"name":"statistics_viewCount","nullable":true,"type":"long"},{"metadata":{},"name":"totalDurationSeconds","nullable":true,"type":"float"}],"type":"struct"},"tableIdentifier":null}],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>contentDetails_duration</th>\n      <th>id</th>\n      <th>snippet_channelId</th>\n      <th>snippet_channelTitle</th>\n      <th>snippet_publishedAt</th>\n      <th>snippet_title</th>\n      <th>statistics_commentCount</th>\n      <th>statistics_dislikeCount</th>\n      <th>statistics_viewCount</th>\n      <th>totalDurationSeconds</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>PT3M33S</td>\n      <td>t1l8Z6gLPzo</td>\n      <td>UCUERSOitwgUq_37kGslN96w</td>\n      <td>VOLO</td>\n      <td>2013-07-22T12:09:11Z</td>\n      <td>VOLO. \"L'air d'un con\"</td>\n      <td>38</td>\n      <td>26</td>\n      <td>223172</td>\n      <td>213.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>PT7M46S</td>\n      <td>we5gzZq5Avg</td>\n      <td>UCson549gpvRhPnJ3Whs5onA</td>\n      <td>LongWayToDream</td>\n      <td>2012-03-17T08:34:30Z</td>\n      <td>Julian Jeweil - Air Conditionné</td>\n      <td>2</td>\n      <td>3</td>\n      <td>13409</td>\n      <td>466.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>PT3M7S</td>\n      <td>49esza4eiK4</td>\n      <td>UCcHYZ8Ez4gG_2bHEuBL8IfQ</td>\n      <td>Downtown Records</td>\n      <td>2007-09-08T02:02:07Z</td>\n      <td>Justice - D.A.N.C.E</td>\n      <td>3168</td>\n      <td>780</td>\n      <td>10106655</td>\n      <td>187.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>PT3M43S</td>\n      <td>BoO6LfR7ca0</td>\n      <td>UCQ0wLCF7u23gZKJkHFs1Tpg</td>\n      <td>Music Is Our Drug</td>\n      <td>2014-01-24T12:52:38Z</td>\n      <td>Gramatik - Torture (feat. Eric Krasno)</td>\n      <td>6</td>\n      <td>0</td>\n      <td>29153</td>\n      <td>223.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>PT5M</td>\n      <td>DaH4W1rY9us</td>\n      <td>UCJsTMPZxYD-Q3kEmL4Qijpg</td>\n      <td>Harvey Pearson</td>\n      <td>2012-12-02T12:41:13Z</td>\n      <td>Ben Howard - Oats In The Water</td>\n      <td>5303</td>\n      <td>1784</td>\n      <td>16488714</td>\n      <td>300.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":"<div class=\"ansiout\">Out[16]: </div>","removedWidgets":[],"addedWidgets":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>contentDetails_duration</th>\n      <th>id</th>\n      <th>snippet_channelId</th>\n      <th>snippet_channelTitle</th>\n      <th>snippet_publishedAt</th>\n      <th>snippet_title</th>\n      <th>statistics_commentCount</th>\n      <th>statistics_dislikeCount</th>\n      <th>statistics_viewCount</th>\n      <th>totalDurationSeconds</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>PT3M33S</td>\n      <td>t1l8Z6gLPzo</td>\n      <td>UCUERSOitwgUq_37kGslN96w</td>\n      <td>VOLO</td>\n      <td>2013-07-22T12:09:11Z</td>\n      <td>VOLO. \"L'air d'un con\"</td>\n      <td>38</td>\n      <td>26</td>\n      <td>223172</td>\n      <td>213.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>PT7M46S</td>\n      <td>we5gzZq5Avg</td>\n      <td>UCson549gpvRhPnJ3Whs5onA</td>\n      <td>LongWayToDream</td>\n      <td>2012-03-17T08:34:30Z</td>\n      <td>Julian Jeweil - Air Conditionné</td>\n      <td>2</td>\n      <td>3</td>\n      <td>13409</td>\n      <td>466.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>PT3M7S</td>\n      <td>49esza4eiK4</td>\n      <td>UCcHYZ8Ez4gG_2bHEuBL8IfQ</td>\n      <td>Downtown Records</td>\n      <td>2007-09-08T02:02:07Z</td>\n      <td>Justice - D.A.N.C.E</td>\n      <td>3168</td>\n      <td>780</td>\n      <td>10106655</td>\n      <td>187.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>PT3M43S</td>\n      <td>BoO6LfR7ca0</td>\n      <td>UCQ0wLCF7u23gZKJkHFs1Tpg</td>\n      <td>Music Is Our Drug</td>\n      <td>2014-01-24T12:52:38Z</td>\n      <td>Gramatik - Torture (feat. Eric Krasno)</td>\n      <td>6</td>\n      <td>0</td>\n      <td>29153</td>\n      <td>223.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>PT5M</td>\n      <td>DaH4W1rY9us</td>\n      <td>UCJsTMPZxYD-Q3kEmL4Qijpg</td>\n      <td>Harvey Pearson</td>\n      <td>2012-12-02T12:41:13Z</td>\n      <td>Ben Howard - Oats In The Water</td>\n      <td>5303</td>\n      <td>1784</td>\n      <td>16488714</td>\n      <td>300.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["We'll make sure we don't have any null values."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0d14c769-c55b-4723-935b-aa85f5c4f247"}}},{"cell_type":"code","source":["# TODO: Count the number of null values in the `totalDurationSeconds` column of `songs_with_total_seconds_duration`\n### BEGIN STRIP ###\nsongs_output \\\n  .select(F.sum(F.col('totalDurationSeconds').isNull().astype('int'))) \\\n  .rdd.map(lambda r: r[0]).first()\n### END STRIP ###"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f2c88af-0b92-4ee7-96fb-59e210ff95e5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[17]: 0</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[17]: 0</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**If you got 0 null values, good job, you made it!**\n\nThis is great progress, but we used a `UDF`: this is not very good performances wise.  \nIt would be better to implement this using PySpark functions.\n\nYou can now use this new variable to perform more analysis. **Good luck!**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3917ac77-171c-4763-9c96-7a83264c8e74"}}},{"cell_type":"markdown","source":["## Bonus\nImplement what we just did, but using standard PySpark (e.g. without using UDF)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d84b20d1-20fc-4d8c-8522-29f777f58476"}}},{"cell_type":"code","source":["# BONUS: compute totalDurationSeconds using only PySpark functions\n### BEGIN STRIP ###\nfrom itertools import accumulate\nfrom operator import mul\nfrom typing import List\n\nfrom pyspark.sql import Column\n\nSECONDS_PER_SECOND: int = 1\nSECONDS_PER_MINUTE: int = 60\nMINUTES_PER_HOUR: int = 60\nHOURS_PER_DAY: int = 24\nDAYS_PER_WEEK: int = 7\nWEEKS_PER_YEAR: int = 52\n\ndef extractISO8601Duration(iso8601_duration_col: str) -> Column:\n  \"\"\"Extract total duration in seconds from an ISO8601 duration column. \"\"\"\n  \n  ISO8601_PATTERN: str = (r'P(?:(\\d+)Y)?(?:(\\d+)W)?(?:(\\d+)D)?'\n                          r'T(?:(\\d+)H)?(?:(\\d+)M)?(?:(\\d+)S)?')\n  MULTIPLIERS: List[int] = (SECONDS_PER_SECOND, SECONDS_PER_MINUTE, MINUTES_PER_HOUR,\n                            HOURS_PER_DAY, DAYS_PER_WEEK, WEEKS_PER_YEAR)\n  GROUPS: List[int] = list(reversed(list(accumulate(MULTIPLIERS, mul))))\n  \n  def _regexp_extract(col_name: str, pattern: str, idx: int) -> Column:\n    \"\"\"Similar to PySpark's `regexp_extract` but returns 0 instead of empty string if no match.\n    \"\"\"\n    extract_col = F.regexp_extract(col_name, pattern, idx)\n    return F.when(extract_col != '', extract_col).otherwise(0)\n  \n  return reduce(add, (_regexp_extract(iso8601_duration_col, ISO8601_PATTERN, g + 1) * F.lit(m)\n                      for g, m in enumerate(GROUPS)))\n\nsongs_output = songs.withColumn('totalSeconds', extractISO8601Duration(DURATION_COL))\nsongs_output.limit(5).toPandas()\n### END STRIP ###"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c19c18fe-a24c-45ed-95b6-d307ae9f7151"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"02-Extracting_duration","dashboards":[],"language":"python","widgets":{},"notebookOrigID":2518071720507922}},"nbformat":4,"nbformat_minor":0}
