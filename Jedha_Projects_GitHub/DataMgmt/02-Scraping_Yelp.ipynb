{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"02-Scraping_Yelp_solution.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"HQMfwL3ej1jf","colab_type":"text"},"source":["# Scraping Yelp\n","\n","The aim of this exercise is to allow a user to make an automatic search on [Yelp](https://www.yelp.fr/) and store the results in a .json file. You will be guided through the different steps : making a form request with search keywords, parsing the search results, crawling all the result pages and storing the results into a file.\n","\n","**As scrapy is not made to launch several crawler processes in the same script, you will have to restart your notebook's kernel before completing each question !**"]},{"cell_type":"markdown","metadata":{"id":"kZR_swbFj1ji","colab_type":"text"},"source":["1. Create a class `YelpSpider(scrapy.Spider)` with `start_urls = ['https://www.yelp.fr/']`. In this class, define a `parse(self, response)` method that automatically fills Yelp's homepage form with : \"restaurant japonais\" as search keywords and \"Paris\" as search location. Then, define another method `after_search(self, response)` that parses the first page of results, and yields the name and url of each search result. Finally, declare a `CrawlerProcess` that will store the results in a file named `\"restaurant_japonais-paris.json\"`."]},{"cell_type":"code","metadata":{"id":"vM4qKu5Oj1jk","colab_type":"code","colab":{}},"source":["import os\n","import logging\n","\n","import scrapy\n","from scrapy.crawler import CrawlerProcess"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B1ot9KzFj1ju","colab_type":"code","colab":{}},"source":["class YelpSpider(scrapy.Spider):\n","    # Name of your spider\n","    name = \"yelp\"\n","\n","    # Starting URL\n","    start_urls = ['https://www.yelp.fr/']\n","\n","    # Parse function for form request\n","    def parse(self, response):\n","        # FormRequest used to make a search in Paris\n","        return scrapy.FormRequest.from_response(\n","            response,\n","            formdata={'find_desc': 'restaurant japonais', 'find_loc': 'paris'},\n","            callback=self.after_search\n","        )\n","\n","    # Callback used after login\n","    def after_search(self, response):\n","        \n","        results = response.css('h4 a')\n","        \n","        for r in results:\n","            yield {\n","                'name': r.css('::text').get(),\n","                'url': \"https://www.yelp.fr\" + r.attrib[\"href\"]\n","            }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1gk9qBPLj1jy","colab_type":"code","colab":{},"outputId":"1be91312-ff89-42f0-c954-87f4c31e8a71"},"source":["# Name of the file where the results will be saved\n","filename = \"restaurant_japonais-paris.json\"\n","\n","# If file already exists, delete it before crawling (because Scrapy will concatenate the last and new results otherwise)\n","if filename in os.listdir('results/'):\n","        os.remove('results/' + filename)\n","\n","# Declare a new CrawlerProcess with some settings\n","process = CrawlerProcess(settings = {\n","    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n","    'LOG_LEVEL': logging.INFO,\n","    \"FEEDS\": {\n","        'results/' + filename : {\"format\": \"json\"},\n","    }\n","})\n","\n","# Start the crawling using the spider you defined above\n","process.crawl(YelpSpider)\n","process.start()\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2020-09-01 14:19:36 [scrapy.utils.log] INFO: Scrapy 2.3.0 started (bot: scrapybot)\n","2020-09-01 14:19:36 [scrapy.utils.log] INFO: Versions: lxml 4.5.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.8.5 | packaged by conda-forge | (default, Jul 31 2020, 02:39:48) - [GCC 7.5.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 3.0, Platform Linux-4.19.112+-x86_64-with-glibc2.10\n","2020-09-01 14:19:36 [scrapy.crawler] INFO: Overridden settings:\n","{'LOG_LEVEL': 20,\n"," 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n","2020-09-01 14:19:36 [scrapy.extensions.telnet] INFO: Telnet Password: 07c0818057376a18\n","2020-09-01 14:19:36 [scrapy.middleware] INFO: Enabled extensions:\n","['scrapy.extensions.corestats.CoreStats',\n"," 'scrapy.extensions.telnet.TelnetConsole',\n"," 'scrapy.extensions.memusage.MemoryUsage',\n"," 'scrapy.extensions.feedexport.FeedExporter',\n"," 'scrapy.extensions.logstats.LogStats']\n","2020-09-01 14:19:37 [scrapy.middleware] INFO: Enabled downloader middlewares:\n","['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n"," 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n"," 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n"," 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n"," 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n"," 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n"," 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n"," 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n"," 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n","2020-09-01 14:19:37 [scrapy.middleware] INFO: Enabled spider middlewares:\n","['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n"," 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n"," 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n"," 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n"," 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n","2020-09-01 14:19:37 [scrapy.middleware] INFO: Enabled item pipelines:\n","[]\n","2020-09-01 14:19:37 [scrapy.core.engine] INFO: Spider opened\n","2020-09-01 14:19:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n","2020-09-01 14:19:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n","2020-09-01 14:19:41 [scrapy.core.engine] INFO: Closing spider (finished)\n","2020-09-01 14:19:41 [scrapy.extensions.feedexport] INFO: Stored json feed (30 items) in: results/restaurant_japonais-paris.json\n","2020-09-01 14:19:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n","{'downloader/request_bytes': 1929,\n"," 'downloader/request_count': 3,\n"," 'downloader/request_method_count/GET': 3,\n"," 'downloader/response_bytes': 138054,\n"," 'downloader/response_count': 3,\n"," 'downloader/response_status_count/200': 2,\n"," 'downloader/response_status_count/302': 1,\n"," 'elapsed_time_seconds': 4.128968,\n"," 'finish_reason': 'finished',\n"," 'finish_time': datetime.datetime(2020, 9, 1, 14, 19, 41, 151692),\n"," 'item_scraped_count': 30,\n"," 'log_count/INFO': 11,\n"," 'memusage/max': 75784192,\n"," 'memusage/startup': 75784192,\n"," 'request_depth_max': 1,\n"," 'response_received_count': 2,\n"," 'scheduler/dequeued': 3,\n"," 'scheduler/dequeued/memory': 3,\n"," 'scheduler/enqueued': 3,\n"," 'scheduler/enqueued/memory': 3,\n"," 'start_time': datetime.datetime(2020, 9, 1, 14, 19, 37, 22724)}\n","2020-09-01 14:19:41 [scrapy.core.engine] INFO: Spider closed (finished)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"141RGMqCj1j5","colab_type":"text"},"source":["2. Once you've managed to get the first page's results in \"restaurant_japonais-paris.json\", complete the `after_search(self,response)` method to crawl the different result pages, such that all the search results will be stored in the file `\"restaurant_japonais-paris.json\"`. Restart your notebook's kernel, execute the new `CrawlerProcess` and check that all the search results (and not only the first page) are now stored in the file."]},{"cell_type":"code","metadata":{"id":"YwWcUOh4j1j6","colab_type":"code","colab":{}},"source":["import os\n","import logging\n","\n","import scrapy\n","from scrapy.crawler import CrawlerProcess"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rd8l3BhDj1j-","colab_type":"code","colab":{}},"source":["class YelpSpider(scrapy.Spider):\n","    # Name of your spider\n","    name = \"yelp\"\n","\n","    # Starting URL\n","    start_urls = ['https://www.yelp.fr/']\n","\n","    # Parse function for form request\n","    def parse(self, response):\n","        # FormRequest used to make a search in Paris\n","        return scrapy.FormRequest.from_response(\n","            response,\n","            formdata={'find_desc': 'restaurant japonais', 'find_loc': 'paris'},\n","            callback=self.after_search\n","        )\n","\n","    # Callback used after login\n","    def after_search(self, response):\n","        \n","        results = response.css('h4 a')\n","        \n","        for r in results:\n","            yield {\n","                'name': r.css('::text').get(),\n","                'url': \"https://www.yelp.fr\" + r.attrib[\"href\"]\n","            }\n","            \n","        # Select the NEXT button and store it in next_page\n","        try:\n","            next_page = response.css('a.next-link').attrib[\"href\"]\n","        except KeyError:\n","            logging.info('No next page. Terminating crawling process.')\n","        else:\n","            yield response.follow(next_page, callback=self.after_search)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4k0pl3sDj1kE","colab_type":"code","colab":{},"outputId":"22e3544a-ee4e-4525-eb9e-6de85f175f2e"},"source":["# Name of the file where the results will be saved\n","filename = \"restaurant_japonais-paris.json\"\n","\n","# If file already exists, delete it before crawling (because Scrapy will concatenate the last and new results otherwise)\n","if filename in os.listdir('results/'):\n","        os.remove('results/' + filename)\n","\n","# Declare a new CrawlerProcess with some settings\n","process = CrawlerProcess(settings = {\n","    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n","    'LOG_LEVEL': logging.INFO,\n","    \"FEEDS\": {\n","        'results/' + filename : {\"format\": \"json\"},\n","    }\n","})\n","\n","# Start the crawling using the spider you defined above\n","process.crawl(YelpSpider)\n","process.start()\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2020-09-01 14:24:22 [scrapy.utils.log] INFO: Scrapy 2.3.0 started (bot: scrapybot)\n","2020-09-01 14:24:22 [scrapy.utils.log] INFO: Versions: lxml 4.5.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.8.5 | packaged by conda-forge | (default, Jul 31 2020, 02:39:48) - [GCC 7.5.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 3.0, Platform Linux-4.19.112+-x86_64-with-glibc2.10\n","2020-09-01 14:24:22 [scrapy.crawler] INFO: Overridden settings:\n","{'LOG_LEVEL': 20,\n"," 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n","2020-09-01 14:24:22 [scrapy.extensions.telnet] INFO: Telnet Password: d744b107c4cf10b2\n","2020-09-01 14:24:22 [scrapy.middleware] INFO: Enabled extensions:\n","['scrapy.extensions.corestats.CoreStats',\n"," 'scrapy.extensions.telnet.TelnetConsole',\n"," 'scrapy.extensions.memusage.MemoryUsage',\n"," 'scrapy.extensions.feedexport.FeedExporter',\n"," 'scrapy.extensions.logstats.LogStats']\n","2020-09-01 14:24:23 [scrapy.middleware] INFO: Enabled downloader middlewares:\n","['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n"," 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n"," 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n"," 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n"," 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n"," 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n"," 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n"," 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n"," 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n","2020-09-01 14:24:23 [scrapy.middleware] INFO: Enabled spider middlewares:\n","['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n"," 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n"," 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n"," 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n"," 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n","2020-09-01 14:24:23 [scrapy.middleware] INFO: Enabled item pipelines:\n","[]\n","2020-09-01 14:24:23 [scrapy.core.engine] INFO: Spider opened\n","2020-09-01 14:24:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n","2020-09-01 14:24:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n","2020-09-01 14:24:47 [root] INFO: No next page. Terminating crawling process.\n","2020-09-01 14:24:47 [scrapy.core.engine] INFO: Closing spider (finished)\n","2020-09-01 14:24:47 [scrapy.extensions.feedexport] INFO: Stored json feed (240 items) in: results/restaurant_japonais-paris.json\n","2020-09-01 14:24:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n","{'downloader/request_bytes': 12467,\n"," 'downloader/request_count': 10,\n"," 'downloader/request_method_count/GET': 10,\n"," 'downloader/response_bytes': 702946,\n"," 'downloader/response_count': 10,\n"," 'downloader/response_status_count/200': 9,\n"," 'downloader/response_status_count/302': 1,\n"," 'elapsed_time_seconds': 24.157405,\n"," 'finish_reason': 'finished',\n"," 'finish_time': datetime.datetime(2020, 9, 1, 14, 24, 47, 172858),\n"," 'item_scraped_count': 240,\n"," 'log_count/INFO': 12,\n"," 'memusage/max': 75759616,\n"," 'memusage/startup': 75759616,\n"," 'request_depth_max': 8,\n"," 'response_received_count': 9,\n"," 'scheduler/dequeued': 10,\n"," 'scheduler/dequeued/memory': 10,\n"," 'scheduler/enqueued': 10,\n"," 'scheduler/enqueued/memory': 10,\n"," 'start_time': datetime.datetime(2020, 9, 1, 14, 24, 23, 15453)}\n","2020-09-01 14:24:47 [scrapy.core.engine] INFO: Spider closed (finished)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"SoPiOH2Yj1kP","colab_type":"text"},"source":["Congrats, you've just made the proof of concept of making an automated search on Yelp with Scrapy ! Now, let's improve the script such that it will allow the user to make any search at any location üòé\n","\n","> Indented block\n","\n","> Indented block\n","\n","> Indented block\n","\n","> Indented block\n","\n","> Indented block\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","3. Use python's `input()` function to ask the user which keywords and location he would like to use, and save them into two variables : `search_keywords` and `search_location`. Then, change the `parse(self, response)` method such that it fills Yelp's form with user-defined keywords and location. Finally, change the `CrawlerProcess` such that it stores the results in a file named with the following format : \"search_keywords-location.json\". \n","\n","Try your search engine with different keywords and locations ‚úåÔ∏è"]},{"cell_type":"code","metadata":{"id":"j97jTGZZj1kU","colab_type":"code","colab":{}},"source":["import os\n","import logging\n","\n","import scrapy\n","from scrapy.crawler import CrawlerProcess"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CWL8-TTUj1kg","colab_type":"code","colab":{},"outputId":"b88edbb8-b3de-441c-82d3-3e0fdb5fc1dc"},"source":["print(\"Welcome in the automated Yelp search engine !\")\n","\n","search_keywords = input(\"Please enter your search keywords : \")\n","search_location = input(\"Please enter the name of the city : \")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Welcome in the automated Yelp search engine !\n"],"name":"stdout"},{"output_type":"stream","text":["Please enter your search keywords :  spa\n","Please enter the name of the city :  Grenoble\n"],"name":"stdin"}]},{"cell_type":"code","metadata":{"id":"NFz-VFwLj1ko","colab_type":"code","colab":{}},"source":["class YelpSpider(scrapy.Spider):\n","    # Name of your spider\n","    name = \"yelp\"\n","\n","    # Starting URL\n","    start_urls = ['https://www.yelp.fr/']\n","\n","    # Parse function for form request\n","    def parse(self, response):\n","        # FormRequest used to make a search\n","        return scrapy.FormRequest.from_response(\n","            response,\n","            formdata={'find_desc': search_keywords, 'find_loc': search_location},\n","            callback=self.after_search\n","        )\n","\n","    # Callback used after login\n","    def after_search(self, response):\n","        \n","        results = response.css('h4 a')\n","        \n","        for r in results:\n","            yield {\n","                'name': r.css('::text').get(),\n","                'url': \"https://www.yelp.fr\" + r.attrib[\"href\"]\n","            }\n","            \n","        # Select the NEXT button and store it in next_page\n","        try:\n","            next_page = response.css('a.next-link').attrib[\"href\"]\n","        except KeyError:\n","            logging.info('No next page. Terminating crawling process.')\n","        else:\n","            yield response.follow(next_page, callback=self.after_search)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IhO5CEMaj1kr","colab_type":"code","colab":{},"outputId":"11e45be6-bdc0-483c-c4da-9d812dfeb949"},"source":["# Name of the file where the results will be saved\n","filename = search_keywords.replace(\" \", \"_\") + \"-\" + search_location + \".json\"\n","\n","# If file already exists, delete it before crawling (because Scrapy will concatenate the last and new results otherwise)\n","if filename in os.listdir('results/'):\n","        os.remove('results/' + filename)\n","\n","# Declare a new CrawlerProcess with some settings\n","process = CrawlerProcess(settings = {\n","    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n","    'LOG_LEVEL': logging.INFO,\n","    \"FEEDS\": {\n","        'results/' + filename : {\"format\": \"json\"},\n","    }\n","})\n","\n","# Start the crawling using the spider you defined above\n","process.crawl(YelpSpider)\n","process.start()\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2020-09-01 14:31:45 [scrapy.utils.log] INFO: Scrapy 2.3.0 started (bot: scrapybot)\n","2020-09-01 14:31:45 [scrapy.utils.log] INFO: Versions: lxml 4.5.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.8.5 | packaged by conda-forge | (default, Jul 31 2020, 02:39:48) - [GCC 7.5.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 3.0, Platform Linux-4.19.112+-x86_64-with-glibc2.10\n","2020-09-01 14:31:45 [scrapy.crawler] INFO: Overridden settings:\n","{'LOG_LEVEL': 20,\n"," 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n","2020-09-01 14:31:45 [scrapy.extensions.telnet] INFO: Telnet Password: f7dddfab2d16ca6e\n","2020-09-01 14:31:45 [scrapy.middleware] INFO: Enabled extensions:\n","['scrapy.extensions.corestats.CoreStats',\n"," 'scrapy.extensions.telnet.TelnetConsole',\n"," 'scrapy.extensions.memusage.MemoryUsage',\n"," 'scrapy.extensions.feedexport.FeedExporter',\n"," 'scrapy.extensions.logstats.LogStats']\n","2020-09-01 14:31:46 [scrapy.middleware] INFO: Enabled downloader middlewares:\n","['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n"," 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n"," 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n"," 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n"," 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n"," 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n"," 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n"," 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n"," 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n","2020-09-01 14:31:46 [scrapy.middleware] INFO: Enabled spider middlewares:\n","['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n"," 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n"," 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n"," 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n"," 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n","2020-09-01 14:31:46 [scrapy.middleware] INFO: Enabled item pipelines:\n","[]\n","2020-09-01 14:31:46 [scrapy.core.engine] INFO: Spider opened\n","2020-09-01 14:31:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n","2020-09-01 14:31:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n","2020-09-01 14:31:50 [root] INFO: No next page. Terminating crawling process.\n","2020-09-01 14:31:50 [scrapy.core.engine] INFO: Closing spider (finished)\n","2020-09-01 14:31:50 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: results/spa-Grenoble.json\n","2020-09-01 14:31:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n","{'downloader/request_bytes': 1986,\n"," 'downloader/request_count': 3,\n"," 'downloader/request_method_count/GET': 3,\n"," 'downloader/response_bytes': 121408,\n"," 'downloader/response_count': 3,\n"," 'downloader/response_status_count/200': 2,\n"," 'downloader/response_status_count/302': 1,\n"," 'elapsed_time_seconds': 4.849391,\n"," 'finish_reason': 'finished',\n"," 'finish_time': datetime.datetime(2020, 9, 1, 14, 31, 50, 872261),\n"," 'item_scraped_count': 10,\n"," 'log_count/INFO': 12,\n"," 'memusage/max': 75993088,\n"," 'memusage/startup': 75993088,\n"," 'request_depth_max': 1,\n"," 'response_received_count': 2,\n"," 'scheduler/dequeued': 3,\n"," 'scheduler/dequeued/memory': 3,\n"," 'scheduler/enqueued': 3,\n"," 'scheduler/enqueued/memory': 3,\n"," 'start_time': datetime.datetime(2020, 9, 1, 14, 31, 46, 22870)}\n","2020-09-01 14:31:50 [scrapy.core.engine] INFO: Spider closed (finished)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"bZpOkfW6j1kw","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}